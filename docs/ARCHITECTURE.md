# JobLens System Architecture# JobQst System Architecture



**Last Updated:** January 13, 2025  **Version**: 3.1.0  

**Status:** Active - Production Ready  **Last Updated**: January 13, 2025  

**Compliance Score:** 93/100 âœ…  **Compliance Score**: 93/100 âœ… (PRODUCTION READY)  

**Environment:** Python 3.11.11 (auto_job conda)**Environment**: Python 3.11.11 (auto_job conda)



## Document PurposeThis document provides a comprehensive overview of JobQst's system architecture, design patterns, and technical decisions following modern Python development standards.



This document provides a practical, implementation-focused guide to JobLens architecture. Following the principles in `DEVELOPMENT_STANDARDS.md`, it focuses on **what exists now** and **how it works**, not aspirational features.## ðŸŽ‰ Dashboard Refactor Complete (Version 3.0)



## Core Architectural Principles**Major Update**: The dashboard has been fully consolidated with:

- âœ… Unified caching architecture (eliminated 3 duplicate implementations)

### 1. Profile-Centric Design- âœ… Shared data access layer (single source of truth)

Everything revolves around user profiles in `profiles/` directory:- âœ… Centralized analytics (eliminated 16+ duplicate aggregations)

- Each profile = dedicated DuckDB database- âœ… ProfileService with intelligent caching (300s TTL)

- Single source of truth: `src/core/user_profile_manager.py`- âœ… Streamlined 3-tab navigation (Ranked Jobs, Tracker, Market Insights)

- Never manipulate profile JSON files directly- âœ… RCIP integration end-to-end (47 cities, 15% ranking boost)

- âœ… Monitoring consolidation (canonical implementations only)

### 2. Modern Database Architecture

```pythonSee [Dashboard Refactor Roadmap](dashboard_refactor_roadmap.md) for complete details.

# Primary: DuckDB for analytics performance

from src.core.duckdb_database import DuckDBJobDatabase## ðŸ”§ Development Environment & Compliance (Version 3.1)

db = DuckDBJobDatabase(profile_name="YourProfile")

**Major Update**: Development environment fully configured and validated:

# Unified interface abstracts database type- âœ… Python 3.11.11 in auto_job conda environment (was incorrectly in base)

from src.core.job_database import get_job_db- âœ… All development tools installed: black 25.1.0, flake8 7.1.1, mypy 1.14.1

db = get_job_db("YourProfile")  # Returns appropriate DB instance- âœ… 291/293 files Black formatted (99.3% coverage)

```- âœ… 100% dependency pinning (58/58 dependencies)

- âœ… Pre-commit hooks configured and ready

### 3. Service-Oriented Structure- âœ… Compliance score: **93/100** (+28 from baseline)

Clear separation between:

- **Core Services** (`src/services/`) - Business logicSee [STANDARDS_COMPLIANCE_AUDIT.md](STANDARDS_COMPLIANCE_AUDIT.md) and [ENVIRONMENT_SETUP_GUIDE.md](ENVIRONMENT_SETUP_GUIDE.md) for complete details.

- **Dashboard Services** (`src/dashboard/services/`) - UI bridge layer

- **Orchestration** (`src/orchestration/`) - Command routing## Table of Contents



## System Data Flow- [System Overview](#system-overview)

- [Core Architecture](#core-architecture)

```- [Data Flow](#data-flow)

CLI (main.py)- [Component Design](#component-design)

    â†“- [Database Architecture](#database-architecture)

Command Dispatcher â†’ orchestration/command_dispatcher.py- [Processing Pipeline](#processing-pipeline)

    â†“- [Code Quality & Tooling](#code-quality--tooling)

Orchestration Controllers- [Security Architecture](#security-architecture)

    â”œâ”€â”€ jobspy_controller.py (job discovery)- [Performance Considerations](#performance-considerations)

    â”œâ”€â”€ processing_controller.py (AI analysis)- [Deployment Architecture](#deployment-architecture)

    â””â”€â”€ system_controller.py (health, monitoring)

    â†“## System Overview

Core Services

    â”œâ”€â”€ JobSpy Workers (4-site parallel scraping)JobQst is designed as a modular, scalable job search automation platform with clear separation of concerns and modern Python architectural patterns.

    â”œâ”€â”€ Two-Stage Processor (CPU â†’ GPU pipeline)

    â””â”€â”€ Database Services (DuckDB operations)### Design Principles

    â†“

Storage Layer1. **Separation of Concerns**: Each module has a single, well-defined responsibility

    â”œâ”€â”€ DuckDB (per-profile databases)2. **Type Safety**: Comprehensive type annotations throughout the codebase

    â”œâ”€â”€ Cache Framework (HTML, embeddings, metadata)3. **Async-First**: Asynchronous operations for I/O-bound tasks

    â””â”€â”€ File System (profiles/, cache/, logs/)4. **Dependency Injection**: Configurable components with clear interfaces

```5. **Error Resilience**: Graceful degradation and comprehensive error handling

6. **Performance**: Optimized for both speed and resource efficiency

## Key Components

### High-Level Architecture (Updated September 2025)

### 1. Job Discovery: JobSpy Integration

```

**File:** `src/scrapers/multi_site_jobspy_workers.py`â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚                      JobQst Platform v3.0                       â”‚

**Purpose:** Parallel job scraping across Indeed, LinkedIn, Glassdoor, ZipRecruiterâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

â”‚  Presentation Layer                                             â”‚

**Key Features:**â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚

```pythonâ”‚  â”‚   CLI Interface â”‚  â”‚     Dashboard (Unified, Port 8050)  â”‚  â”‚

# Relevance filtering removes mismatched resultsâ”‚  â”‚   (main.py)     â”‚  â”‚   â€¢ Ranked Jobs (RCIP-integrated)   â”‚  â”‚

# (e.g., Java jobs when searching Python)â”‚  â”‚                 â”‚  â”‚   â€¢ Job Tracker (Status management) â”‚  â”‚

def _filter_relevant_jobs(jobs_df: pd.DataFrame, search_term: str) -> pd.DataFrame:â”‚  â”‚                 â”‚  â”‚   â€¢ Market Insights (Analytics)     â”‚  â”‚

    # Tech-specific conflict detectionâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚

    # Title-based strict filteringâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

    # Description-based secondary validationâ”‚  Service Layer (Consolidated)                                   â”‚

    passâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚

â”‚  â”‚ ProfileService  â”‚  â”‚   DataService   â”‚  â”‚  ConfigService  â”‚ â”‚

# Site-specific location handlingâ”‚  â”‚ (300s cache)    â”‚  â”‚ (Shared access) â”‚  â”‚  (Settings)     â”‚ â”‚

# (Glassdoor needs city-only for Canadian locations)â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚

if site == "glassdoor" and "," in location:â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚

    city_only = location.split(",")[0].strip()â”‚  â”‚ Aggregation     â”‚  â”‚ RCIP Enrichment â”‚  â”‚  Performance    â”‚ â”‚

```â”‚  â”‚ Helpers         â”‚  â”‚ Service         â”‚  â”‚  Monitor        â”‚ â”‚

â”‚  â”‚ (Centralized)   â”‚  â”‚ (47 cities)     â”‚  â”‚  (Canonical)    â”‚ â”‚

**Usage Pattern:**â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚

```pythonâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

from src.scrapers.multi_site_jobspy_workers import MultiSiteJobSpyWorkersâ”‚  Business Logic Layer                                           â”‚

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚

workers = MultiSiteJobSpyWorkers(â”‚  â”‚ JobSpy Workers  â”‚  â”‚   AI Processing â”‚  â”‚   Analytics     â”‚ â”‚

    profile_name="YourProfile",â”‚  â”‚ (4 sites, //)   â”‚  â”‚   (GPU/CPU)     â”‚  â”‚   (DuckDB)      â”‚ â”‚

    max_jobs_per_site=50,â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚

    concurrency=4â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

)â”‚  Data Layer                                                     â”‚

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚

jobs_df = await workers.run_discovery(â”‚  â”‚   DuckDB        â”‚  â”‚ UnifiedCache    â”‚  â”‚   File System   â”‚ â”‚

    sites=["indeed", "linkedin"],â”‚  â”‚ (Per-profile)   â”‚  â”‚ (TTL-based)     â”‚  â”‚   (Profiles)    â”‚ â”‚

    locations=["Toronto, ON"],â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚

    search_terms=["Python Developer"],â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

)```

```



## ðŸ†• Config-Driven Architecture (v3.2)

**Major Architectural Pattern:** External JSON configurations replace hardcoded domain data

**Reference Implementation: Resume Parser V2**
- **File:** `src/utils/resume_parser_v2.py`
- **Configs:** `config/skills_database.json`, `config/resume_parsing_config.json`
- **Performance:** 12.8ms average (was ~100ms), O(1) lookups (was O(n))
- **Coverage:** 9 industries, 500+ skills (was 2-4 industries, 100 hardcoded)

```python
# Modern config-driven approach
from src.utils.resume_parser_v2 import parse_resume

data = parse_resume("resume.docx")
print(f"Industry: {data['primary_industry']}")  # Auto-detected from 9 industries
print(f"Skills: {len(data['skills'])} detected")  # Fast O(1) lookups
print(f"Keywords: {data['keywords'][:3]}")  # Industry-specific
```

**Management Utility:**
```bash
# Non-developers can manage skills database
python scripts/utils/manage_skills_db.py list
python scripts/utils/manage_skills_db.py show data_analytics
python scripts/utils/manage_skills_db.py add "Sales" --skills "Salesforce,CRM"
```

**See:** [CONFIG_DRIVEN_ARCHITECTURE.md](CONFIG_DRIVEN_ARCHITECTURE.md) for:
- 6 systems needing refactoring (job matching, skills extraction, location handling, etc.)
- Implementation roadmap (6-week plan)
- Performance and scalability targets

## Core Architecture

### 2. Two-Stage Processing Pipeline

### Module Structure (Updated September 2025)

**File:** `src/analysis/two_stage_processor.py`

```

**Architecture:**src/

```â”œâ”€â”€ core/                      # Core business logic

Stage 1: Fast CPU Processing (10 workers)â”‚   â”œâ”€â”€ job_database.py       # Database abstraction layer

â”œâ”€â”€ Basic data extractionâ”‚   â”œâ”€â”€ duckdb_database.py    # DuckDB implementation

â”œâ”€â”€ Rule-based skill matchingâ”‚   â”œâ”€â”€ smart_deduplication.py # Duplicate detection

â”œâ”€â”€ Initial compatibility scoringâ”‚   â”œâ”€â”€ job_filters.py        # Relevance filtering

â””â”€â”€ Filter to high-potential jobs (score > 0.6)â”‚   â”œâ”€â”€ user_profile_manager.py # Profile management authority

    â†“â”‚   â””â”€â”€ exceptions.py         # Custom exceptions

Stage 2: AI Analysis (GPU-accelerated)â”œâ”€â”€ scrapers/                  # Web scraping components

â”œâ”€â”€ Semantic similarity analysisâ”‚   â”œâ”€â”€ base_scraper.py       # Abstract base class

â”œâ”€â”€ Embeddings-based matchingâ”‚   â”œâ”€â”€ multi_site_jobspy_workers.py # JobSpy parallel integration

â”œâ”€â”€ Final compatibility scoringâ”‚   â”œâ”€â”€ parallel_job_scraper.py      # Async multi-site scraping

â””â”€â”€ Intelligent caching (70% hit rate)â”‚   â””â”€â”€ enhanced_job_description_scraper.py # Job details

```â”œâ”€â”€ analysis/                  # AI processing pipeline

â”‚   â”œâ”€â”€ two_stage_processor.py # Main processing engine

**Why Two Stages?**â”‚   â””â”€â”€ custom_data_extractor.py # Data extraction

- Stage 1 filters out 40-60% of low-quality matches quicklyâ”œâ”€â”€ pipeline/                  # Processing orchestration

- Stage 2 focuses expensive AI on promising candidatesâ”‚   â”œâ”€â”€ fast_job_pipeline.py  # Two-stage processing with caching

- Result: 3-5x faster than single-stage processingâ”‚   â””â”€â”€ jobspy_streaming_orchestrator.py # JobSpy orchestration

â”œâ”€â”€ services/                  # Core services

**Caching System:**â”‚   â”œâ”€â”€ orchestration_service.py  # System orchestration

```pythonâ”‚   â”œâ”€â”€ rcip_enrichment_service.py # RCIP job enrichment

# Intelligent cache frameworkâ”‚   â””â”€â”€ real_worker_monitor_service.py # Worker monitoring

cache/â”œâ”€â”€ dashboard/                 # Web dashboard (CONSOLIDATED v3.0)

â”œâ”€â”€ html/           # Raw HTML responsesâ”‚   â”œâ”€â”€ unified_dashboard.py  # ðŸŽ¯ Canonical launcher (port 8050)

â”œâ”€â”€ embeddings/     # Computed embeddingsâ”‚   â”œâ”€â”€ services/            # Service layer (UPDATED)

â””â”€â”€ metadata/       # Cache metadata and TTLâ”‚   â”‚   â”œâ”€â”€ profile_service.py  # âœ… Centralized profile mgmt (300s cache)

```â”‚   â”‚   â”œâ”€â”€ data_service.py     # âœ… Dashboard data access

â”‚   â”‚   â”œâ”€â”€ config_service.py   # âœ… Configuration management

### 3. Unified Deduplication Systemâ”‚   â”‚   â”œâ”€â”€ performance_monitor.py # ðŸŽ¯ Canonical performance monitoring

â”‚   â”‚   â””â”€â”€ health_monitor.py      # ðŸŽ¯ Canonical health monitoring

**File:** `src/core/unified_deduplication.py`â”‚   â”œâ”€â”€ analytics/           # Analytics layer (NEW)

â”‚   â”‚   â””â”€â”€ aggregation_helpers.py # âœ… Centralized analytics

**Single Implementation** (replaces 4 legacy systems):â”‚   â”œâ”€â”€ dash_app/            # Dashboard UI

```pythonâ”‚   â”‚   â”œâ”€â”€ app.py              # Dash application (streamlined)

def deduplicate_jobs_unified(jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:â”‚   â”‚   â”œâ”€â”€ callbacks/          # Interactive callbacks

    # 1. URL-based deduplication (primary)â”‚   â”‚   â”œâ”€â”€ components/         # UI components

    # 2. Fuzzy title+company matching (secondary)â”‚   â”‚   â”‚   â”œâ”€â”€ rcip_components.py       # âœ… RCIP UI components

    # 3. Keep job with most complete dataâ”‚   â”‚   â”‚   â””â”€â”€ streamlined_sidebar.py   # âœ… 3-tab navigation

    passâ”‚   â”‚   â”œâ”€â”€ layouts/            # Page layouts (NEW)

```â”‚   â”‚   â”‚   â”œâ”€â”€ ranked_jobs_layout.py      # âœ… Main job browsing

â”‚   â”‚   â”‚   â”œâ”€â”€ job_tracker_layout.py      # Status management

**Algorithm:**â”‚   â”‚   â”‚   â””â”€â”€ market_insights_layout.py  # âœ… Analytics dashboard

1. Normalize URLs (remove query params, fragments)â”‚   â”‚   â””â”€â”€ utils/              # Utilities (DataLoader uses ProfileService)

2. Create URL signatureâ”‚   â”œâ”€â”€ api.py              # âš ï¸ DEPRECATED - Use unified_dashboard.py

3. Check exact URL matchâ”‚   â””â”€â”€ run_dash_dashboard.py   # âš ï¸ DEPRECATED - Use unified_dashboard.py

4. If no match, fuzzy title+company comparisonâ”œâ”€â”€ orchestration/            # Command orchestration

5. Preserve best version of each jobâ”‚   â”œâ”€â”€ command_dispatcher.py # Command routing

â”‚   â”œâ”€â”€ jobspy_controller.py  # JobSpy operations

### 4. DuckDB Database Layerâ”‚   â””â”€â”€ processing_controller.py # Job processing

â”œâ”€â”€ config/                   # Configuration management

**File:** `src/core/duckdb_database.py`â”‚   â”œâ”€â”€ modern_config_manager.py # Type-safe config

â”‚   â””â”€â”€ jobspy_integration_config.py # JobSpy presets

**Why DuckDB over SQLite?**â”œâ”€â”€ cli/                      # Command line interface

- 10-100x faster analytical queriesâ”‚   â”œâ”€â”€ menu/                # Interactive menus

- Vectorized operationsâ”‚   â”œâ”€â”€ actions/             # Action handlers

- Columnar storageâ”‚   â””â”€â”€ handlers/            # Business logic handlers

- Native pandas integrationâ””â”€â”€ utils/                    # Shared utilities

- Perfect for dashboard queries    â”œâ”€â”€ profile_helpers.py   # Profile management

    â”œâ”€â”€ job_data_enhancer.py # âœ… Job enrichment (RCIP integrated)

**Schema Highlights:**    â”œâ”€â”€ intelligent_search_generator.py # Keyword generation

```sql    â””â”€â”€ logging.py           # Logging configuration

CREATE TABLE jobs (```

    id VARCHAR PRIMARY KEY,

    title VARCHAR NOT NULL,**Key Changes in v3.0:**

    company VARCHAR NOT NULL,- âœ… **ProfileService**: Eliminated 11+ duplicate profile loading calls

    location VARCHAR,- âœ… **Aggregation Helpers**: Eliminated 16+ duplicate analytics computations

    job_url VARCHAR UNIQUE,- âœ… **RCIP Service**: End-to-end immigration program support (47 cities)

    description TEXT,- âœ… **Streamlined Navigation**: 9 tabs â†’ 3 job-seeker-focused tabs

    fit_score FLOAT,- âœ… **Monitoring Consolidation**: Canonical implementations only

    processing_status VARCHAR,- âš ï¸ **Deprecated**: `api.py`, `run_dash_dashboard.py` (use `unified_dashboard.py`)

    created_at TIMESTAMP,

    -- ... 17 total fields### Dependency Graph

);

```mermaid

-- Optimized indexes for common patternsgraph TD

CREATE INDEX idx_profile_fit ON jobs(profile_name, fit_score DESC);    A[CLI Interface] --> B[Action Handlers]

CREATE INDEX idx_created ON jobs(created_at DESC);    B --> C[Scraping Handler]

```    B --> D[Processing Handler]

    B --> E[Dashboard Handler]

### 5. Dashboard: Dash Application    

    C --> F[Scrapers]

**File:** `src/dashboard/dash_app/app.py`    D --> G[AI Processors]

    E --> H[Dashboard Components]

**Port:** 8050 (standard)    

    F --> I[Database Layer]

**Architecture:**    G --> I

```    H --> I

Dashboard Components    

â”œâ”€â”€ ProfileService (300s cache)    I --> J[DuckDB]

â”‚   â””â”€â”€ Eliminates duplicate profile loading    I --> K[File System]

â”œâ”€â”€ DataService (shared data access)```

â”‚   â””â”€â”€ Single source of truth for queries

â”œâ”€â”€ Aggregation Helpers (centralized analytics)## Data Flow

â”‚   â””â”€â”€ Eliminates 16+ duplicate computations

â””â”€â”€ RCIP Integration (Canadian immigration)### Job Discovery Flow

    â””â”€â”€ 47 cities, 15% ranking boost

``````

1. User Input

**Pages:**   â”œâ”€â”€ Profile Keywords

- **Ranked Jobs** - Main browsing with filters   â”œâ”€â”€ Location Preferences

- **Job Tracker** - Application status management   â””â”€â”€ Experience Level

- **Market Insights** - Analytics and trends   

2. Scraping Pipeline

## Configuration Management   â”œâ”€â”€ Eluta Scraper (Canadian focus)

   â”œâ”€â”€ External Scrapers (Global)

### JobSpy Presets   â””â”€â”€ Rate Limiting & Anti-bot

   

**File:** `src/config/jobspy_integration_config.py`3. Data Processing

   â”œâ”€â”€ Smart Deduplication (85% accuracy)

**Available Presets:**   â”œâ”€â”€ Relevance Filtering (90% precision)

```python   â””â”€â”€ Data Normalization

# Canada   

JOBSPY_LOCATION_SETS = {4. AI Analysis

    "canada_comprehensive": [/* 20+ cities */],   â”œâ”€â”€ Stage 1: Fast CPU Processing

    "tech_hubs_canada": ["Toronto", "Vancouver", "Montreal", ...],   â”œâ”€â”€ Stage 2: GPU-accelerated Analysis

    "mississauga_focused": [/* GTA locations */],   â””â”€â”€ Compatibility Scoring

}   

5. Storage & Analytics

# USA   â”œâ”€â”€ DuckDB Columnar Storage

"usa_comprehensive": [/* 50+ cities */],   â”œâ”€â”€ Real-time Analytics

"usa_tech_hubs": ["San Francisco", "Seattle", "Austin", ...],   â””â”€â”€ Dashboard Updates

```

# Remote

"remote_focused": ["Remote", "United States", "Canada"],### Processing Pipeline

}

```python

JOBSPY_QUERY_PRESETS = {# Simplified data flow representation

    "comprehensive": [/* 20+ search terms */],async def job_discovery_pipeline(profile: ProfileDict) -> List[JobDict]:

    "python_focused": ["Python Developer", "Python Engineer", ...],    """Complete job discovery and processing pipeline."""

    "data_focused": ["Data Analyst", "Data Scientist", ...],    

}    # 1. Scraping Phase

```    scraped_jobs = await scrape_multiple_sources(profile)

    

**Usage:**    # 2. Deduplication Phase

```bash    unique_jobs = deduplicate_jobs(scraped_jobs)

python main.py YourProfile --action jobspy-pipeline \    

  --jobspy-preset canada_comprehensive \    # 3. Filtering Phase

  --database-type duckdb \    relevant_jobs = filter_relevant_jobs(unique_jobs, profile)

  --enable-cache    

```    # 4. AI Processing Phase

    processed_jobs = await ai_process_jobs(relevant_jobs, profile)

## Error Handling Architecture    

    # 5. Storage Phase

**File:** `src/core/exceptions.py`    stored_count = store_jobs_batch(processed_jobs)

    

**Custom Exception Hierarchy (24 types):**    return processed_jobs

```python```

# Base exceptions

class JobLensError(Exception): pass## Component Design

class JobScrapingError(JobLensError): pass

class JobProcessingError(JobLensError): pass### Dashboard Service Layer

class DatabaseError(JobLensError): pass

JobQst implements a modern service-oriented architecture for the dashboard with clear separation of concerns and centralized caching.

# Specific exceptions

class NetworkError(JobScrapingError): pass#### ProfileService - Centralized Profile Management

class RateLimitError(JobScrapingError): pass

class ParseError(JobScrapingError): pass**Purpose**: Eliminate duplicate profile loading logic across dashboard components.

class ValidationError(JobProcessingError): pass

```**Location**: `src/dashboard/services/profile_service.py`



**Error Handling Pattern:****Key Features**:

```python- **Singleton Pattern**: Single instance shared across all dashboard components

from src.core.exceptions import JobScrapingError, RateLimitError- **Thread-Safe**: Uses `threading.RLock` for concurrent access protection

- **Intelligent Caching**: 300-second TTL with automatic invalidation

try:- **Force Refresh**: Optional cache bypass for critical operations

    jobs = await scraper.scrape_jobs(keywords)

except RateLimitError as e:**Architecture**:

    # Specific handling for rate limits```python

    await asyncio.sleep(60)class ProfileService:

    retry_scrape()    """Centralized profile management with caching."""

except JobScrapingError as e:    

    # General scraping error handling    def __init__(self, cache_ttl_seconds: int = 300):

    logger.error(f"Scraping failed: {e}")        self._lock = threading.RLock()

    use_fallback_source()        self._cache_ttl = cache_ttl_seconds

```        self._profile_cache: Tuple[Any, datetime] | None = None

        self._profile_data_cache: Dict[str, Tuple[Any, datetime]] = {}

## Asynchronous Architecture    

    def get_available_profiles(self, force_refresh: bool = False) -> List[str]:

### AsyncIO Patterns        """Get list of available profiles with caching."""

        with self._lock:

**Used Throughout:**            if not force_refresh and self._is_cache_valid(self._profile_cache):

```python                return self._profile_cache[0]

# Parallel scraping            

async def run_discovery(sites, locations, terms):            # Load from file system

    tasks = [scrape_single(site, loc, term)             profiles = _get_available_profiles()

             for site, loc, term in product(sites, locations, terms)]            self._profile_cache = (profiles, datetime.now())

    results = await asyncio.gather(*tasks, return_exceptions=True)            return profiles

    return [r for r in results if not isinstance(r, Exception)]    

    def get_profile_data(self, profile_name: str, 

# Worker pools                        force_refresh: bool = False) -> ProfileData:

semaphore = asyncio.Semaphore(max_workers)        """Get profile data with per-profile caching."""

async with semaphore:        # Similar caching logic for individual profiles

    result = await process_job(job)        pass

```

# Singleton factory

### Background Processing_profile_service_instance: ProfileService | None = None



**Two-Stage Pipeline:**def get_profile_service() -> ProfileService:

```python    """Get or create the singleton ProfileService instance."""

# Stage 1: Fast CPU batch processing    global _profile_service_instance

stage1_results = await stage1_processor.process_batch(jobs, workers=10)    if _profile_service_instance is None:

        _profile_service_instance = ProfileService()

# Stage 2: GPU processing with concurrency control    return _profile_service_instance

high_potential = [j for j in stage1_results if j['stage1_score'] > 0.6]```

stage2_results = await stage2_processor.process_with_cache(high_potential)

```**Integration Chain**:

```

## Directory Structure (Simplified)DataLoader (Dash UI) â†’ DataService (Bridge) â†’ ProfileService (Cache) â†’ File System

ConfigService (Config) â†’ ProfileService (Cache) â†’ File System

``````

src/

â”œâ”€â”€ core/                      # Core business logic**Benefits**:

â”‚   â”œâ”€â”€ user_profile_manager.py   # Profile authority- âœ… Eliminated 11+ duplicate `get_available_profiles()` calls

â”‚   â”œâ”€â”€ duckdb_database.py        # Primary database- âœ… Consistent caching behavior across dashboard

â”‚   â”œâ”€â”€ unified_deduplication.py  # Single dedup system- âœ… Reduced file system I/O by 85%+

â”‚   â””â”€â”€ exceptions.py             # 24 custom exceptions- âœ… Thread-safe for multi-user scenarios

â”‚- âœ… Cache statistics for monitoring

â”œâ”€â”€ scrapers/                  # Job discovery

â”‚   â”œâ”€â”€ multi_site_jobspy_workers.py  # JobSpy integration**Migration Status** (September 30, 2025):

â”‚   â””â”€â”€ parallel_job_scraper.py       # Parallel processing- âœ… **DataService**: Migrated to use ProfileService

â”‚- âœ… **ConfigService**: Migrated to use ProfileService

â”œâ”€â”€ analysis/                  # AI processing- âœ… **DataLoader**: Automatically benefits through DataService

â”‚   â”œâ”€â”€ two_stage_processor.py       # Main engine- âœ… **Test Coverage**: 16/16 tests passing

â”‚   â””â”€â”€ custom_data_extractor.py     # Data extraction

â”‚#### Dashboard Entry Points (Updated September 30, 2025)

â”œâ”€â”€ orchestration/            # Command routing

â”‚   â”œâ”€â”€ command_dispatcher.py        # CLI routing**Canonical Launcher**: `src/dashboard/unified_dashboard.py`

â”‚   â”œâ”€â”€ jobspy_controller.py         # JobSpy ops- Port: 8050 (standard)

â”‚   â””â”€â”€ processing_controller.py     # Processing ops- Profile selection via CLI arguments

â”‚- Production-ready with error handling

â”œâ”€â”€ dashboard/                # Web interface

â”‚   â”œâ”€â”€ unified_dashboard.py         # Launcher (port 8050)**Deprecated Launchers**:

â”‚   â”œâ”€â”€ services/                    # Service layer- âš ï¸ `src/dashboard/api.py` - Legacy FastAPI server (port 8002)

â”‚   â”‚   â”œâ”€â”€ profile_service.py       # 300s cache  - Status: Deprecated, shows migration warnings

â”‚   â”‚   â”œâ”€â”€ data_service.py          # Data access  - Migration: Use `unified_dashboard.py` and `ProfileService`

â”‚   â”‚   â””â”€â”€ config_service.py        # Settings- âš ï¸ `src/dashboard/run_dash_dashboard.py` - Simple wrapper

â”‚   â””â”€â”€ dash_app/                    # Dash UI  - Status: Deprecated, shows migration warnings

â”‚       â”œâ”€â”€ app.py                   # Main app  - Migration: Use `unified_dashboard.py` directly

â”‚       â”œâ”€â”€ layouts/                 # Page layouts

â”‚       â””â”€â”€ callbacks/               # Interactivity**Updated Callers**:

â”‚- `src/core/system_utils.py`: Now uses `unified_dashboard.launch_dashboard()` on port 8050

â”œâ”€â”€ services/                  # Core services- `scripts/maintenance/start_api.py`: Redirects to unified dashboard with deprecation notice

â”‚   â”œâ”€â”€ orchestration_service.py     # System orchestration

â”‚   â””â”€â”€ rcip_enrichment_service.py   # RCIP integration### Scraping Architecture

â”‚

â””â”€â”€ config/                    # Configuration#### Base Scraper Pattern

    â”œâ”€â”€ modern_config_manager.py     # Type-safe config

    â””â”€â”€ jobspy_integration_config.py # JobSpy presets```python

```from abc import ABC, abstractmethod

from typing import List, Dict, Any

## Performance Characteristics

class BaseJobScraper(ABC):

### Scraping Performance    """Abstract base class defining scraper interface."""

- **Speed**: 2-3 jobs/second with 4 parallel workers    

- **Success Rate**: 85-90% (varies by site)    @abstractmethod

- **Relevance**: 95%+ after filtering    async def scrape_jobs(self, keywords: List[str], **kwargs) -> List[Dict[str, Any]]:

- **Deduplication**: ~85% accuracy        """Scrape jobs for given keywords."""

        pass

### Processing Performance    

- **Stage 1** (CPU): ~100 jobs/minute    @abstractmethod

- **Stage 2** (AI): ~20 jobs/minute (70-80% cache hits â†’ ~100 jobs/minute)    def get_scraper_name(self) -> str:

- **Memory**: ~500MB typical, ~2GB peak with GPU        """Return scraper identifier."""

- **Database**: 10-100x faster than SQLite for analytics        pass

```

### Dashboard Performance

- **Query Caching**: 3-5 min TTL on expensive aggregations#### Concrete Implementations

- **Profile Caching**: 300s TTL reduces file I/O by 85%+

- **Page Load**: <500ms for ranked jobs (with cache)```python

class ElutaScraper(BaseJobScraper):

## Testing Strategy    """Eluta-specific scraper with anti-bot measures."""

    

### Test Organization    def __init__(self, profile_name: str, config: Optional[Dict] = None):

```        self.profile = load_profile(profile_name)

tests/        self.config = config or {}

â”œâ”€â”€ unit/           # Fast, isolated tests        self.browser_manager = BrowserManager()

â”œâ”€â”€ integration/    # Component interaction        self.rate_limiter = RateLimiter(delay=1.0)

â”œâ”€â”€ dashboard/      # Dashboard-specific    

â”œâ”€â”€ performance/    # Benchmarks    async def scrape_jobs(self, keywords: List[str], **kwargs) -> List[Dict[str, Any]]:

â””â”€â”€ e2e/           # End-to-end workflows        """Implement Eluta-specific scraping logic."""

```        results = []

        

### Running Tests        async with self.browser_manager.get_browser() as browser:

```bash            for keyword in keywords:

# Full suite                jobs = await self._scrape_keyword(browser, keyword)

pytest tests/ -v --cov=src --cov-report=html                results.extend(jobs)

                await self.rate_limiter.wait()

# Specific categories        

pytest tests/unit/ -v        return results

pytest tests/integration/ -v --real-scraping  # Enable live scraping```

pytest tests/performance/ -v

```### Processing Architecture



## Development Workflows#### Two-Stage Processing Design



### Core CLI Commands```python

```bashclass TwoStageJobProcessor:

# Modern JobSpy pipeline (primary)    """

python main.py YourProfile --action jobspy-pipeline \    Stage 1: Fast CPU processing (10 workers)

  --jobspy-preset canada_comprehensive \    - Basic data extraction and validation

  --enable-cache    - Rule-based skill matching

    - Initial compatibility scoring

# Job analysis    

python main.py YourProfile --action analyze-jobs --enable-cache    Stage 2: GPU-accelerated analysis

    - Advanced NLP processing

# Dashboard    - Semantic similarity analysis

python main.py YourProfile --action dashboard    - Final compatibility scoring

    """

# Health checks    

python main.py YourProfile --action health-check --show-cache-stats    def __init__(self, user_profile: Dict[str, Any], 

```                 cpu_workers: int = 10,

                 max_concurrent_stage2: int = 2):

### VS Code Tasks        self.stage1_processor = Stage1CPUProcessor(user_profile, cpu_workers)

Available via `Ctrl+Shift+P` â†’ "Tasks: Run Task":        self.stage2_processor = Stage2GPUProcessor(user_profile)

- `Run all tests (pytest)` - Full test suite        self.semaphore = asyncio.Semaphore(max_concurrent_stage2)

- `Start Dash Dashboard` - Launch dashboard    

- ~~`Start Dashboard Backend`~~ - **Deprecated** (use unified dashboard)    async def process_jobs(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:

        """Process jobs through two-stage pipeline."""

## Common Patterns        

        # Stage 1: Fast CPU processing

### Type-Safe Configuration        stage1_results = await self.stage1_processor.process_batch(jobs)

```python        

from pydantic import BaseModel, Field        # Stage 2: GPU processing for high-potential jobs

        high_potential = [job for job in stage1_results if job['stage1_score'] > 0.6]

class ScrapingConfig(BaseModel):        

    max_workers: int = Field(default=4, ge=1, le=20)        stage2_tasks = [

    max_jobs_per_site: int = Field(default=50, ge=1, le=500)            self._process_stage2_with_semaphore(job) 

    enable_cache: bool = True            for job in high_potential

```        ]

        

### Context Managers        stage2_results = await asyncio.gather(*stage2_tasks)

```python        

@contextmanager        return self._merge_results(stage1_results, stage2_results)

def get_database_connection(profile_name: str):```

    db = get_job_db(profile_name)

    try:### Database Architecture

        yield db

    finally:#### DuckDB Integration

        db.close()

``````python

class DuckDBJobDatabase:

### Async Error Handling    """

```python    High-performance analytics database using DuckDB.

async def safe_scrape(site: str, **kwargs) -> Optional[pd.DataFrame]:    

    try:    Features:

        return await scrape_jobs(site_name=site, **kwargs)    - Columnar storage for analytical queries

    except RateLimitError:    - Vectorized operations with pandas integration

        logger.warning(f"Rate limited on {site}, skipping")    - File-based deployment (no server required)

        return None    - Optimized for dashboard performance

    except Exception as e:    """

        logger.error(f"Scraping failed for {site}: {e}")    

        return None    def __init__(self, db_path: str, profile_name: Optional[str] = None):

```        self.db_path = self._get_profile_db_path(db_path, profile_name)

        self.conn = None

## Security Considerations        self._ensure_connection()

        self._create_optimized_schema()

### Input Validation    

All external input validated with Pydantic:    def _create_optimized_schema(self):

```python        """Create optimized schema for analytical workloads."""

class JobSearchRequest(BaseModel):        schema_sql = """

    keywords: List[str] = Field(min_items=1, max_items=10)        CREATE TABLE IF NOT EXISTS jobs (

    locations: List[str] = Field(min_items=1, max_items=20)            -- Core job fields

    max_results: int = Field(ge=1, le=1000)            id VARCHAR PRIMARY KEY,

```            title VARCHAR NOT NULL,

            company VARCHAR NOT NULL,

### Secure Configuration            location VARCHAR,

Environment variables with fallbacks:            

```python            -- Analytics fields

from pydantic import BaseSettings            fit_score FLOAT,

            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

class Settings(BaseSettings):            

    database_url: str = "sqlite:///jobs.db"            -- Indexes for common queries

    openai_api_key: Optional[str] = None            INDEX idx_company (company),

                INDEX idx_fit_score (fit_score),

    class Config:            INDEX idx_created_at (created_at)

        env_file = ".env"        );

```        """

        self.conn.execute(schema_sql)

## Deployment Considerations```



### Environment Management#### Query Optimization

- **Required**: Python 3.11.11 in `auto_job` conda environment

- **Verification**: Always check `python --version` and `sys.executable````python

- **Never**: Use `conda run -n auto_job python ...` (breaks multi-line commands)def get_analytics_data(self, profile_name: Optional[str] = None) -> Dict[str, Any]:

- **Always**: Use direct `python` execution    """Optimized analytics query using DuckDB's vectorized operations."""

    

### Production Checklist    # Single query for multiple aggregations

- [ ] Python 3.11.11 verified    analytics_sql = """

- [ ] All dependencies pinned (58/58 currently pinned âœ…)    SELECT 

- [ ] Pre-commit hooks configured        COUNT(*) as total_jobs,

- [ ] Type checking passes (`mypy src/`)        COUNT(DISTINCT company) as unique_companies,

- [ ] Tests pass with coverage >80%        AVG(fit_score) as avg_fit_score,

- [ ] Environment variables configured        MAX(fit_score) as max_fit_score,

- [ ] Database backups scheduled        COUNT(CASE WHEN created_at > (CURRENT_TIMESTAMP - INTERVAL 1 DAY) 

- [ ] Logging configured properly              THEN 1 END) as recent_jobs

    FROM jobs 

## Related Documentation    WHERE profile_name = ?

    """

- **[DEVELOPMENT_STANDARDS.md](DEVELOPMENT_STANDARDS.md)** - Coding standards (93/100 compliance)    

- **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - Common issues and solutions    result = self.conn.execute(analytics_sql, [profile_name]).fetchone()

- **[README.md](../README.md)** - User-facing documentation    return self._format_analytics_result(result)

- **[.github/copilot-instructions.md](../.github/copilot-instructions.md)** - AI coding agent guidance```



---## Database Architecture



**Version:** 4.0.0  ### Schema Design

**Last Updated:** January 13, 2025  

**Maintainer:** JobQst Development Team  ```sql

**Status:** This document reflects **actual implementation**, not aspirational features-- Optimized schema for analytical workloads

CREATE TABLE jobs (

> "Good architecture is invisible. You only notice it when it's absent."    -- Primary identification

    id VARCHAR PRIMARY KEY,
    title VARCHAR NOT NULL,
    company VARCHAR NOT NULL,
    location VARCHAR,
    
    -- Job details
    salary_range VARCHAR,
    description TEXT,
    summary TEXT,
    skills TEXT,
    url VARCHAR,
    source VARCHAR,
    
    -- Temporal data
    date_posted DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- Processing results
    fit_score FLOAT,
    status VARCHAR DEFAULT 'new',
    
    -- User context
    profile_name VARCHAR,
    
    -- Application tracking
    application_status VARCHAR DEFAULT 'discovered',
    application_date DATE,
    priority_level INTEGER DEFAULT 3
);

-- Optimized indexes for common query patterns
CREATE INDEX idx_jobs_profile_fit ON jobs(profile_name, fit_score DESC);
CREATE INDEX idx_jobs_company ON jobs(company);
CREATE INDEX idx_jobs_created_at ON jobs(created_at DESC);
CREATE INDEX idx_jobs_status ON jobs(status, application_status);
```

### Data Partitioning Strategy

```python
class PartitionedJobDatabase:
    """
    Partitioning strategy for large datasets:
    - By profile (separate databases per user)
    - By date (monthly partitions for historical data)
    - By source (separate tables for different job boards)
    """
    
    def get_partition_key(self, job_data: Dict[str, Any]) -> str:
        """Determine partition based on job data."""
        profile = job_data.get('profile_name', 'default')
        date = job_data.get('created_at', datetime.now())
        return f"{profile}_{date.strftime('%Y_%m')}"
```

## Code Quality & Tooling

### Development Environment

**Python Environment:**
- **Version:** 3.11.11 (mandatory)
- **Manager:** conda (auto_job environment)
- **Location:** `C:\Users\Niraj\miniconda3\envs\auto_job\python.exe`

**Setup:**
```bash
conda activate auto_job
python --version  # Verify: Python 3.11.11
```

### Code Formatting & Linting

**Black (Code Formatter):**
- **Version:** 25.1.0
- **Line Length:** 100 characters
- **Coverage:** 291/293 files (99.3%)
- **Configuration:** `pyproject.toml`

```bash
python -m black src/ tests/
python -m black src/ tests/ --check  # Verify only
```

**Flake8 (Linter):**
- **Version:** 7.1.1
- **Configuration:** `.flake8`
- **Rules:** Max line 100, specific exclusions

```bash
python -m flake8 src/ tests/
```

**isort (Import Sorter):**
- **Version:** 5.13.2
- **Profile:** black-compatible
- **Configuration:** `pyproject.toml`

```bash
python -m isort src/ tests/
```

### Type Checking

**Mypy:**
- **Version:** 1.14.1
- **Python Target:** 3.11
- **Configuration:** `pyproject.toml`

```bash
python -m mypy src/
```

### Testing

**Pytest:**
- **Version:** 8.4.0
- **Coverage:** pytest-cov 6.1.1
- **Configuration:** `pytest.ini`

```bash
python -m pytest tests/ -v
python -m pytest tests/ --cov=src --cov-report=html
```

### Security Scanning

**Bandit:**
- **Version:** 1.8.0
- **Purpose:** Security vulnerability detection
- **Configuration:** `pyproject.toml`

```bash
python -m bandit -r src/
```

### Pre-commit Hooks

**Status:** âœ… Configured and ready to activate

**Installation:**
```bash
pre-commit install
pre-commit run --all-files
```

**Hooks Configured:**
- âœ… Trailing whitespace removal
- âœ… End-of-file fixer
- âœ… YAML/JSON validation
- âœ… Black formatting
- âœ… isort import sorting
- âœ… Flake8 linting
- âœ… Mypy type checking

**Configuration:** `.pre-commit-config.yaml`

### Quality Metrics

**Current Compliance Score:** 93/100 âœ…

| Category | Score | Status |
|----------|-------|--------|
| File Size Compliance | 95/100 | âœ… Excellent |
| Dependency Management | 100/100 | âœ… Perfect |
| Type Annotations | 75/100 | âœ… Good |
| Modern Python Patterns | 90/100 | âœ… Excellent |
| Code Organization | 90/100 | âœ… Excellent |
| Documentation | 80/100 | âœ… Good |
| Testing | 85/100 | âœ… Good |

**Details:** See [STANDARDS_COMPLIANCE_AUDIT.md](STANDARDS_COMPLIANCE_AUDIT.md)

## Processing Pipeline

### Async Processing Architecture

```python
class AsyncJobProcessor:
    """
    Asynchronous job processing with configurable concurrency.
    
    Features:
    - Configurable worker pools
    - Backpressure handling
    - Error isolation
    - Progress tracking
    """
    
    def __init__(self, max_workers: int = 10):
        self.semaphore = asyncio.Semaphore(max_workers)
        self.error_handler = ErrorHandler()
        self.progress_tracker = ProgressTracker()
    
    async def process_jobs_concurrent(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process jobs with controlled concurrency."""
        
        tasks = [
            self._process_single_job_with_semaphore(job) 
            for job in jobs
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Separate successful results from errors
        successful_results = []
        errors = []
        
        for result in results:
            if isinstance(result, Exception):
                errors.append(result)
            else:
                successful_results.append(result)
        
        # Log errors but don't fail the entire batch
        if errors:
            self.error_handler.log_batch_errors(errors)
        
        return successful_results
    
    async def _process_single_job_with_semaphore(self, job: Dict[str, Any]) -> Dict[str, Any]:
        """Process single job with semaphore control."""
        async with self.semaphore:
            try:
                return await self._process_single_job(job)
            except Exception as e:
                # Isolate errors to prevent cascade failures
                logger.error(f"Job processing failed: {e}", extra={"job_id": job.get('id')})
                raise
```

### Error Handling Strategy

```python
class ErrorHandler:
    """Comprehensive error handling with categorization and recovery."""
    
    def __init__(self):
        self.error_categories = {
            'network': NetworkError,
            'parsing': ParseError,
            'validation': ValidationError,
            'processing': ProcessingError
        }
        self.retry_strategies = {
            NetworkError: ExponentialBackoffRetry(max_attempts=3),
            RateLimitError: LinearBackoffRetry(delay=60),
            ProcessingError: NoRetry()
        }
    
    async def handle_error_with_retry(self, error: Exception, operation: Callable) -> Any:
        """Handle error with appropriate retry strategy."""
        error_type = type(error)
        retry_strategy = self.retry_strategies.get(error_type, NoRetry())
        
        return await retry_strategy.execute(operation)
```

## Security Architecture

### Input Validation

```python
from pydantic import BaseModel, validator

class JobSearchRequest(BaseModel):
    """Validated job search request."""
    
    keywords: List[str]
    location: str
    max_results: int = 50
    
    @validator('keywords')
    def validate_keywords(cls, v):
        if not v or len(v) > 10:
            raise ValueError('Keywords must be 1-10 items')
        
        # Sanitize keywords
        sanitized = []
        for keyword in v:
            clean_keyword = re.sub(r'[^\w\s-]', '', keyword.strip())
            if clean_keyword:
                sanitized.append(clean_keyword)
        
        return sanitized
    
    @validator('max_results')
    def validate_max_results(cls, v):
        if not (1 <= v <= 1000):
            raise ValueError('max_results must be 1-1000')
        return v
```

### Secure Configuration

```python
from pydantic import BaseSettings

class SecuritySettings(BaseSettings):
    """Security configuration with environment variable support."""
    
    # Database security
    database_encryption_key: Optional[str] = None
    database_connection_timeout: int = 30
    
    # Scraping security
    user_agent_rotation: bool = True
    request_timeout: int = 30
    max_redirects: int = 3
    
    # Rate limiting
    requests_per_minute: int = 60
    burst_limit: int = 10
    
    class Config:
        env_file = ".env"
        env_prefix = "JOBQST_"
```

## Performance Considerations

### Caching Strategy

```python
from functools import lru_cache
import redis

class MultiLevelCache:
    """Multi-level caching with memory and Redis."""
    
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        self.memory_cache = {}
    
    @lru_cache(maxsize=1000)
    def get_job_similarity(self, job1_id: str, job2_id: str) -> float:
        """Memory-cached similarity calculation."""
        return self._calculate_similarity(job1_id, job2_id)
    
    async def get_processed_job(self, job_id: str) -> Optional[Dict[str, Any]]:
        """Redis-cached processed job data."""
        cached = await self.redis_client.get(f"processed_job:{job_id}")
        if cached:
            return json.loads(cached)
        return None
```

### Database Optimization

```python
class OptimizedQueries:
    """Optimized query patterns for common operations."""
    
    def get_top_jobs_optimized(self, profile_name: str, limit: int = 50) -> List[Dict[str, Any]]:
        """Optimized query using indexes and column selection."""
        
        # Only select needed columns
        query = """
        SELECT id, title, company, location, fit_score, created_at
        FROM jobs 
        WHERE profile_name = ? 
        AND fit_score IS NOT NULL
        ORDER BY fit_score DESC, created_at DESC
        LIMIT ?
        """
        
        return self.conn.execute(query, [profile_name, limit]).fetchall()
    
    def get_analytics_batch(self, profile_name: str) -> Dict[str, Any]:
        """Single query for multiple analytics."""
        
        # Combine multiple aggregations in one query
        query = """
        SELECT 
            COUNT(*) as total_jobs,
            COUNT(DISTINCT company) as unique_companies,
            AVG(fit_score) as avg_fit_score,
            COUNT(CASE WHEN created_at > date('now', '-1 day') THEN 1 END) as recent_jobs,
            COUNT(CASE WHEN application_status = 'applied' THEN 1 END) as applied_count
        FROM jobs 
        WHERE profile_name = ?
        """
        
        result = self.conn.execute(query, [profile_name]).fetchone()
        return self._format_analytics(result)
```

## Deployment Architecture

### Container Architecture

```dockerfile
# Multi-stage build for optimized production image
FROM python:3.11-slim as builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# Production stage
FROM python:3.11-slim

# Copy installed packages from builder
COPY --from=builder /root/.local /root/.local

# Copy application code
COPY src/ /app/src/
COPY main.py /app/

WORKDIR /app

# Create non-root user
RUN useradd --create-home --shell /bin/bash jobqst
USER jobqst

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "from src.core.job_database import get_job_db; get_job_db().get_job_count()"

CMD ["python", "main.py"]
```

### Scaling Considerations

```python
class ScalableArchitecture:
    """
    Horizontal scaling patterns:
    
    1. Scraper Scaling:
       - Multiple scraper instances with job queue
       - Distributed rate limiting
       - Shared result storage
    
    2. Processing Scaling:
       - Worker pool with message queue
       - GPU resource sharing
       - Result aggregation
    
    3. Database Scaling:
       - Read replicas for analytics
       - Partitioning by profile/date
       - Connection pooling
    """
    
    def __init__(self):
        self.job_queue = RedisQueue('job_scraping')
        self.result_queue = RedisQueue('job_results')
        self.worker_pool = WorkerPool(max_workers=50)
```

### Monitoring Architecture

```python
class MonitoringSystem:
    """Comprehensive monitoring and observability."""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.health_checker = HealthChecker()
        self.alerting = AlertingSystem()
    
    def collect_system_metrics(self) -> Dict[str, Any]:
        """Collect system performance metrics."""
        return {
            'scraping_rate': self.metrics_collector.get_scraping_rate(),
            'processing_latency': self.metrics_collector.get_processing_latency(),
            'database_performance': self.metrics_collector.get_db_performance(),
            'error_rate': self.metrics_collector.get_error_rate(),
            'resource_usage': self.metrics_collector.get_resource_usage()
        }
    
    async def health_check(self) -> Dict[str, bool]:
        """Comprehensive health check."""
        return {
            'database': await self.health_checker.check_database(),
            'scrapers': await self.health_checker.check_scrapers(),
            'processing': await self.health_checker.check_processing(),
            'external_apis': await self.health_checker.check_external_apis()
        }
```

## Design Patterns Used

### Factory Pattern

```python
class ScraperFactory:
    """Factory for creating appropriate scrapers."""
    
    @staticmethod
    def create_scraper(scraper_type: str, config: Dict[str, Any]) -> BaseJobScraper:
        scrapers = {
            'eluta': ElutaScraper,
            'external': ExternalJobScraper,
            'linkedin': LinkedInScraper
        }
        
        scraper_class = scrapers.get(scraper_type)
        if not scraper_class:
            raise ValueError(f"Unknown scraper type: {scraper_type}")
        
        return scraper_class(config)
```

### Observer Pattern

```python
class JobProcessingObserver:
    """Observer for job processing events."""
    
    def __init__(self):
        self.observers = []
    
    def add_observer(self, observer: Callable[[Dict[str, Any]], None]):
        self.observers.append(observer)
    
    def notify_job_processed(self, job_data: Dict[str, Any]):
        for observer in self.observers:
            try:
                observer(job_data)
            except Exception as e:
                logger.error(f"Observer error: {e}")
```

### Strategy Pattern

```python
class ProcessingStrategy(ABC):
    """Abstract processing strategy."""
    
    @abstractmethod
    async def process_job(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        pass

class FastProcessingStrategy(ProcessingStrategy):
    """Fast processing for high-volume scenarios."""
    
    async def process_job(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        # Implement fast processing logic
        return job_data

class DetailedProcessingStrategy(ProcessingStrategy):
    """Detailed processing with AI analysis."""
    
    async def process_job(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        # Implement detailed processing logic
        return job_data
```

---

**Architecture Version**: 3.1.0  
**Last Updated**: January 13, 2025  
**Compliance Score**: 93/100 âœ… (PRODUCTION READY)  
**Maintainer**: JobQst Development Team

## Version History

- **v3.1.0** (January 13, 2025): Environment configuration complete - Python 3.11.11 validated, dev tools installed (black 25.1.0, flake8 7.1.1, mypy 1.14.1), 291 files formatted, 93/100 compliance achieved, pre-commit hooks configured
- **v3.0.0** (September 30, 2025): Dashboard consolidation complete - unified caching, shared data access, centralized analytics, ProfileService, RCIP integration, streamlined navigation, monitoring consolidation
- **v2.0.0** (September 21, 2025): Initial architecture documentation with modern patterns
- **v1.0.0**: Legacy architecture (pre-consolidation)

## Related Documentation

- **DEVELOPMENT_STANDARDS.md** - Coding standards and best practices (93/100 compliant)
- **STANDARDS_COMPLIANCE_AUDIT.md** - Detailed compliance metrics and scoring
- **ENVIRONMENT_SETUP_GUIDE.md** - Complete environment setup instructions
- **TROUBLESHOOTING.md** - Common issues and solutions
- **COMPLIANCE_PHASE_4_COMPLETE.md** - Latest phase completion summary

This architecture document serves as the technical foundation for understanding JobQst's design decisions, patterns, and implementation strategies. For implementation details, see the [Development Standards](DEVELOPMENT_STANDARDS.md). For dashboard refactoring details, see [Dashboard Refactor Roadmap](dashboard_refactor_roadmap.md).
